{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Package Code For Link Users For More Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "from IPython.display import Markdown, display # Ipython Function for Displaying the Results\n",
    "def jd_printing(string,val=None): # Function to Make Bold and Italic text over the screen\n",
    "    if val ==None:\n",
    "        display(Markdown(string))\n",
    "    else :\n",
    "        (display(Markdown(string+\"\\t\\t:\\t\\t\"+str(val))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imporitng the Essential Packages For Email Analytics Using Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.4'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os           # Operating System Module inbuilt in Python \n",
    "import pandas as pd # Pandas is inbuilt in anaconda if not install seprately (Note:- Version of Pandas, for this file 0.23.4)\n",
    "# Check Version with Below Command as required \n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Here is Link for Pandas Installation***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://pandas.pydata.org/\"> Pandas_Install_Guide </marquee></a><b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_printing (\"***Here is Link for Pandas Installation***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://pandas.pydata.org/\"> Pandas_Install_Guide </marquee></a><b>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Here is Link for Pandas Site, Documentation How To Use Pandas Function***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><font size=\"5\"> <marquee HEIGHT=50><a href=\"https://pandas.pydata.org/pandas-docs/stable/index.html/\">Pandas_Site </marquee></a><b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_printing(\"***Here is Link for Pandas Site, Documentation How To Use Pandas Function***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50><a href=\"https://pandas.pydata.org/pandas-docs/stable/index.html/\">\\\n",
    "Pandas_Site </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV FILE Using Pandas DataFrame with Hyper Parameter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# File Name is In Which Raw Data Stored.\n",
    "# Note :-  If Raw Data format is different please refer Pandas Site for more detail and use accordingly.\n",
    "# As per this use case i am using read_csv format in pandas.\n",
    "# Note :-  Enconding is Must as its vary data to data for dealing with these kind of issues read file format over web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "***Here is Link for Python Encoding list to Deal with data reading***"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://docs.python.org/2.4/lib/standard-encodings.html\"> Python_Encoding_list </marquee></a><b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_printing(\"***Here is Link for Python Encoding list to Deal with data reading***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://docs.python.org/2.4/lib/standard-encodings.html\"> \\\n",
    "Python_Encoding_list </marquee></a><b>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'inbox_pst.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-403-fd3a3b33f683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFile_Name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"inbox_pst.csv\"\u001b[0m  \u001b[1;31m# Raw Data file Name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0memail_data\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFile_Name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"latin1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Loading CSV into Pandas which RUN over RAM of Machine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mjd_printing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"***Total Records in Data Set***\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'inbox_pst.csv' does not exist"
     ]
    }
   ],
   "source": [
    "File_Name = \"inbox_pst.csv\"  # Raw Data file Name\n",
    "email_data =pd.read_csv(File_Name,encoding=\"latin1\",sep=',') # Loading CSV into Pandas which RUN over RAM of Machine\n",
    "jd_printing(\"***Total Records in Data Set***\",len(email_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over View of Data Set  Using Pandas Inbuilt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.info() # This will tell how many records in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.columns # Columns Present in Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the Pandas Data Frame as per requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.rename(index=str, columns={\"From: (Name)\": \"From\", \"To: (Name)\": \"To\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving Serial Number to Document for Future Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data[\"Serial_Num\"] = [x+1 for x in range(len(email_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Few Step are Part of Data Wrangling or Data Enginering, That Provide you Some insight of Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.head() # Top Five Record View, To See How Data Looks in Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.To.unique()  #  Unique Email Id for TO list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.From.unique()  # Unique Email Id for From list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.To.value_counts() # Count of  Emails as per columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.From.value_counts() #   Count of  Emails as per columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.Subject.value_counts().head() # Duplicate Subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.Subject.value_counts().head() # Duplicate Subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(email_data.Body.value_counts().head()).keys(),list(email_data.Body.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Email Body Content With Same Subject Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duplicate_email = email_data[email_data.duplicated(['Subject', 'Body']).groupby(email_data['Body']).transform('any')]\n",
    "Duplicate_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting List of Duplicate Values in List format for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Droping_list = Duplicate_email.Serial_Num\n",
    "list(Droping_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicate Data From ORIGINAL Data Set with Existing Records as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data = email_data.loc[~email_data['Serial_Num'].isin(list(Droping_list))]\n",
    "len(email_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample View of Duplicate and Removed From Original As well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duplicate_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data[165:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Data From Duplicate Emails\n",
    "### This will be added to Update Clean Data Frame after removing all Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duplicate_email = Duplicate_email.drop_duplicates(subset=[\"Subject\"],keep='first')\n",
    "Duplicate_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Back Unique Value From Duplicate TO Original Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data = email_data.append(Duplicate_email)\n",
    "email_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now From HERE MACHINE LEARNING PART STARTED\n",
    "## Inital Phase is Data Cleaning Or Data Preprocessing of Text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spacy Package Used for Text Preprocessing to Extract Noun and Verb in Our Use Case, Spacy is Widely Use Industry Now Days,\n",
    "Having Good Community over Google and Actively Devloping.\n",
    "Below is Refrence Link of Space and Installation Guidance as Well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # Loading the Spcay English Package for our use case\n",
    "jd_printing('***Spacy Package Has Been Called for Processing the Text data***')\n",
    "jd_printing(\"***Here is Link for Spacy Package For Text Analytics***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://spacy.io/\"> \\\n",
    "SPACY_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing NLTK Package for to perform Text Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stop Words are Non~Revealant Words like (a,an, the,is, when, etc.) which make model complex remove them\n",
    "# Sentance tokenize use  for to break down paragraph into sentances\n",
    "# Word Tokenize use  for to break down sentances into  words\n",
    "# Lemmatizer Used for to get correct form of words as per context of text not the root the root of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "pre_stop_words_list = set(stopwords.words('english'))\n",
    "jd_printing('***NLTK Package Has Been Called for Processing the Text data***')\n",
    "jd_printing(\"***Here is Link for NLTK Package For Text Analytics, For Installation Please Refer the Site***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"http://www.nltk.org/\"> \\\n",
    "NLTK_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below are some Non Relvant Words Encountred in text which is will be added into Stop words list\n",
    "# Post Stop Words will have both NLTK stopword as well as User defined (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_req = [\"--\",\":\",\"\\n\",\"*\",\"\\n\",\"@\",\"ect\",\"m\",\"cc\",\"to\",'as','a','an','the',\"-\",\"a.m.\",\"p.m.\",\"=20\",\"=\",\n",
    "               \"td\",\"iii\",\"www\",\"jpg\",\"http\",\"jpg\",'th',\"msn\",'com','iso','pm','am','a','ll','mw',\n",
    "                        'yahoo','ena',\"eb\",\"xls\",\"st\",\"pdf\",\"outlook\",\"doc\",\"hrweb\",\"et\",\"asp\",\".ect\",\"xx\",\"eol\",\n",
    "                        \"nbsp\",\"txt\",\"ve\",\"el\",\"bg\",\"tx\",\"ut\",\"kt\",\"se\",\"sw\",\"nw\",\"tw\",\"ngx\",\"da\",\"usc\",\"ed\",\"ld\",\"ng\",\n",
    "                        \"wsj\",\"sb\",\"loveect\",\"fw\",\"for\",\"a\",\"of\",\"and\",\"to\",\"in\",\"ndas\",\"ss\",\"ab\",\"plea\",\"ma\",'ke',\n",
    "                        \"ch\",\"jr\",\"mr\",'eml','lme','xom',\"bn\",\"td\",\"nda\",\"hou\"]\n",
    "\n",
    "Post_stop_words_list = list_not_req+list(pre_stop_words_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# From this Stage the Text Cleaning Part is Starting.\n",
    "# This Below Function will remove all URL LINK From The Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text_inside_brackets(text, brackets):\n",
    "    count = [0] * (len(brackets) // 2) # count open/close brackets\n",
    "    saved_chars = [] # save the result to return\n",
    "    text =text.lower()\n",
    "    for character in text:\n",
    "        for i, b in enumerate(brackets):\n",
    "            if character == b: # found bracket\n",
    "                kind, is_close = divmod(i, 2)\n",
    "                count[kind] += (-1)**is_close # `+1`: open, `-1`: close\n",
    "                if count[kind] < 0: # unbalanced bracket\n",
    "                    count[kind] = 0  # keep it\n",
    "                else:  # found bracket to remove\n",
    "                    break\n",
    "        else: # character is not a [balanced] bracket\n",
    "            if not any(count): # outside brackets\n",
    "                saved_chars.append(character)\n",
    "    return ''.join(saved_chars) # Return the String"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This Below Function will Remove Trail Email Context which is Not Use (Like To,From,CC,BCC, Most are the Name Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace(text,Start_Value=\"to:\",End_value =\"subject:\"):\n",
    "    index_ = text.index(Start_Value)\n",
    "    End_   = text.index(End_value)+8\n",
    "    New_text = text[index_:End_]\n",
    "    Final_text = text.replace(New_text,\"\")\n",
    "    del index_\n",
    "    del End_\n",
    "    return Final_text # Return the Clean Trail Email without Non Relavant Stuff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below Funtion is Written  to Tag the words with POS Tagging (Part of Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Space_Tagger(text):\n",
    "    doc =  nlp(text) # Converting the text into Spacy text for POS Tagging\n",
    "    taggeer_ = [(token.text, token.tag_) for token in doc] # One Liner funtion to Return word and POS Tag\n",
    "    return taggeer_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below Funtion is Written to Extract Only NOUN and VERB\n",
    "# if Any URL is still present in data this function will replace with Blank Space which don't effect model\n",
    "# Also Removing Long Words and Short Word which are not actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Used for Regular Expression\n",
    "def Select_NN_VB_Text(Text):\n",
    "    tag_text = [ j[0] for j in Text if j[1].startswith(\"NN\") or j[1] == \"VB\" if j[0] not in Post_stop_words_list ]\n",
    "    tag_text_R = []\n",
    "    for j in tag_text:\n",
    "        if j.endswith(\"txt\") or j.startswith(\"http\"):\n",
    "            j = \"\"\n",
    "            tag_text_R.append(re.sub('[^A-Z,a-z]',\"\",j))\n",
    "        \n",
    "        else:\n",
    "            tag_text_R.append(re.sub('[^A-Z,a-z]',\"\",j))\n",
    "        \n",
    "    \n",
    "     \n",
    "    tag_text = [ x for x in tag_text_R if len(x) > 2  ]\n",
    "    \n",
    "    tag_text = [ x for x in tag_text if len(x) < 15 ]   \n",
    "    \n",
    "    return tag_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Below Fucntion is Calling Four (4) Function to Provide the Required Words for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_Cleaning_Call_All_Above(text):\n",
    "    Body_ = remove_text_inside_brackets(text,brackets=\"<>[]\") # Cleaning URL \n",
    "    for j in range(1,11): # Ten Time Loop Run to Clean the URL From Text With Exception Handling to avoid any bug\n",
    "        try:                 \n",
    "            New_Text = find_and_replace(Body_)\n",
    "            Body_     = New_Text\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    Body_ = Space_Tagger(Body_) # Return the POS Tagging \n",
    "    Body_ = Select_NN_VB_Text(Body_) # Return the Words Which Under for Machine Learning \n",
    "    return Body_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "jd_printing('***Joblib Package have been used for Parallel Computing To Utilize all Core of Machine \\n \\\n",
    "            This will boost up the speed and reduce the of Processing \\\n",
    "            \\n NOTE:-  Prior to Use this Library Please Check you Machine Configuration as well \\\n",
    "            \\n NOTE:- Please Refer Version as well***', joblib.__version__)\n",
    "jd_printing(\"***Here is Link for JOBLIB PACKAGE, For Installation Please Refer the Site***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://joblib.readthedocs.io/en/latest/\"> \\\n",
    "JOBLIB_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "jd_printing('***TQDM is Used for Visualize Progress Bar of Data Processing***')\n",
    "jd_printing(\"***Here is Link for TQDM PACKAGE, For Installation Please Refer the Site***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://github.com/tqdm/tqdm\"> \\\n",
    "TQDM_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Preprocesing the Text Using Above Created Function and Joblib for Fast Processing (GET List of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Body = Parallel(n_jobs=2,backend='threading')(delayed(Text_Cleaning_Call_All_Above)(x)for x in tqdm(email_data.Body))\n",
    "jd_printing('***Pre~processing has been Completed as Goal***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Body[0] # Sample Result of above task please change index value till the data size -1 and have look on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\jkdadhich\\\\Desktop\\\\HR_DEMO_JD\\\\out_put\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Here Machine Learning Task is Starting\n",
    "## Genism Package is using for ML Algorithim for the Use Case\n",
    "### Each Algorithim has their own criteria and results are vary as per selected algo.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In Demo Solution I am using different Technique using my best knowledge.\n",
    "# Run all Algo First and Save the Results for future reference or use case.\n",
    "# Gensim high level Package and Easy to use develop by Radim and his team.\n",
    "# Gensim is widely used for Text analytics with different technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "jd_printing(\"***Here is Link for GENSIM PACKAGE, For Installation Please Refer the Site***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://radimrehurek.com/gensim/\"> \\\n",
    "GENSIM_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model Is Word2VEC one of The Best Model as per  Requirement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Word2vec Implementation :- Two types CBOW & SKIP GRAM\n",
    "# Below is sample of Word2vec Model using Skip Gram.\n",
    "# For more under standing please google the word2vec model (part of neural netword with One hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        Clean_Body,\n",
    "        size=100, # Size equal to Number of Topics & size is the dimensionality of the feature vectors\n",
    "        window=5,  # neighbourhood of a term, in a sentence\n",
    "        min_count=2, # Ignores all words with total frequency lower than this\n",
    "        workers=2, #  Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "        sg=1,    # Skip gram model\n",
    "        negative = 5, #If > 0, negative sampling will be used, \n",
    "                        #the int for negative specifies how many “noise words” \n",
    "                        #should be drawn (usually between 5-20). If set to 0, no negative sampling is used\n",
    "        alpha = 0.25,\n",
    "        iter= 10\n",
    ") \n",
    "jd_printing(\"***Word2Vec Model has been Trained with Given Input***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv.syn0 ## Keys or Words willbe Zero Index and Dimenson will be First index (Word*Size)\n",
    "jd_printing(\"***Shape of Word Vectors Created by Model***\",word_vectors.shape)  # Shape of Vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Note:-  Word Vectors are always Greater in Size with respect to Corpus Size\n",
    "# To Deal with Problem I have applied Technique Average of Vectors (To Reduce Dimesion by averaging them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sent_vectorizer(sent, model):\n",
    "    if len(sent)!=0:\n",
    "        sent_vec =[]\n",
    "        numw = 0\n",
    "        for w in sent:\n",
    "            try:\n",
    "                if numw == 0:\n",
    "                    sent_vec = model[w]\n",
    "                else:\n",
    "                    sent_vec = np.add(sent_vec, model[w])\n",
    "                numw+=1\n",
    "            except:\n",
    "                pass\n",
    "        return np.asarray(sent_vec) / numw \n",
    "    else:\n",
    "        return np.zeros(shape=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over Corpus With Model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_Vector = Parallel(n_jobs=2,backend='threading')(delayed(sent_vectorizer)(sentence, model)\n",
    "                                                       for sentence in tqdm(Clean_Body))\n",
    "jd_printing(\"***Average Vectors are Ready For Clustering***\",len(Corpus_Vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_Vectors = []\n",
    "for sentence in Clean_Body:\n",
    "    Avg_Vectors.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Clustering Comes In Picture \n",
    "### In this case i am using kmean Clustering with Iteration and model will decide by its own\n",
    "#### We can also decided Number of Cluster as per need however best will be machine decision if you are not Sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # Imporitng the Kmeans\n",
    "from sklearn import metrics        # To Check Clustering Performance    \n",
    "jd_printing(\"***Here is Link for Sklearn Kmean, For Installation Please Refer the Site***\")\n",
    "HTML('<b><font size=\"5\"> <marquee HEIGHT=50> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\"> \\\n",
    "Sklearn_Kmean_Package </marquee></a><b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below We are iterating over number of k and checking which number of cluster is good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cluster_range = 5\n",
    "End_Cluster_Range   = 20\n",
    "\n",
    "Dict_Result = {}\n",
    "for K_range in range(start_cluster_range,End_Cluster_Range):\n",
    "    kmeans  = KMeans(n_clusters=K_range)\n",
    "    result_ = kmeans.fit(Avg_Vectors)\n",
    "    clusters = [ i for i in (result_.labels_)]\n",
    "    silhouette_score = metrics.silhouette_score(Corpus_Vector, clusters, metric='euclidean')\n",
    "    Dict_Result[K_range] = silhouette_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_Result # Result In Dictionay Format and Key as Cluster Number and Values are Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator # Calling inbuilt Library \n",
    "# Getting Best Number of As Per Machine Calulation.\n",
    "Best_Cluster_Value = max(Dict_Result.items(), key=operator.itemgetter(1))[0]\n",
    "jd_printing(\"***Number of Cluster For this Data Set is***\",Best_Cluster_Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model as with Best Cluster Values for Future Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans  = KMeans(n_clusters=Best_Cluster_Value)\n",
    "kmean_result = kmeans.fit(Avg_Vectors)\n",
    "Kmean_Labels = [ x for x in kmean_result.labels_]\n",
    "Kmean_Centroid = [x for x in kmean_result.cluster_centers_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data Set or Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # For Ploting\n",
    " \n",
    "from sklearn.manifold import TSNE\n",
    " \n",
    "model_TSNE = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    " \n",
    "Y=model_TSNE.fit_transform(Avg_Vectors)\n",
    " \n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=Kmean_Labels, s=200,alpha=.9)\n",
    " \n",
    "for j in range(len(clusters)):    \n",
    "    plt.annotate(Kmean_Labels[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data_word2vec_kmean = email_data # Copying the Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Data Frame as per Cluster and Cleaned Text\n",
    "email_data_word2vec_kmean[\"K_Label\"] = Kmean_Labels\n",
    "email_data_word2vec_kmean[\"Clean_Word\"] = Clean_Body\n",
    "email_data_word2vec_kmean.to_excel(\"Word2vec.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Address_of_png_files = []\n",
    "for Cls in range(0,max(Kmean_Labels)+1):\n",
    "    Image_Cls = [ y for x,y in zip(email_data_word2vec_kmean.K_Label,email_data_word2vec_kmean.Clean_Word) if x == Cls]\n",
    "    print (len(Image_Cls),Cls)\n",
    "    word_clound = WordCloud(relative_scaling=0.5,stopwords=STOPWORDS)\n",
    "    word_clound.generate(str(Image_Cls))\n",
    "    word_clound.to_file(str(\"K_Label_\")+str(Cls)+str(\".png\"))  \n",
    "    print (\"Image of Cluster Number  Save on Disk \",Cls)\n",
    "    Address_of_png_files.append(os.getcwd()+str(\"\\\\K_Label_\")+str(Cls)+str(\".png\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Address_of_png_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document()\n",
    "for j in Address_of_png_files:\n",
    "    document.add_heading(j)\n",
    "    document.add_picture(j, width=Inches(6.0))\n",
    "    \n",
    "document.save(\"Demo.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.wv.most_similar(positive=['open', 'clustering'], negative=['change'])\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
